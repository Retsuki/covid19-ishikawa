name: scrapingdata

on:
  schedule:
    - cron: '*/5 * * * * '

jobs:
  scraping:
    runs-on: ubuntu-18.04
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python 3.x
        uses: actions/setup-python@v1
        with:
          python-version: '3.7'

      - name: Cache pip
        uses: actions/cache@v1
        with:
          path: ~/.cache/pip 
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraping
        run: python ./tool/scraping.py
      
      - name: Setup Node
        uses: actions/setup-node@v1
        with:
          node-version: '10.x'

      - name: Cache dependencies
        uses: actions/cache@v1
        with:
          path: ~/.cache/yarn
          key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}
          restore-keys: |
            ${{ runner.os }}-yarn-

      - run: yarn install --frozen-lockfile
      - run: yarn run test
      - run: yarn run generate:dev --fail-on-page-error
      - run: "echo \"User-agent: *\nDisallow: /\" > ./dist/robots.txt"

      - name: deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./data
          publish_branch: dev-pages
